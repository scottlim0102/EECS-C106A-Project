{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamza-Kamran/EECS-C106A-Project/blob/main/Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvqiei0yiScn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers\n",
        "from tensorflow import Tensor\n",
        "from tensorflow.linalg import matmul\n",
        "from tensorflow.nn import softmax\n",
        "from tensorflow.math import *\n",
        "import collections\n",
        "import logging\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hamza's model\n",
        "#todos: \n",
        "#confirm unit tests for each unit\n",
        "#fix dimension problems with input in encoder layer\n",
        "#add any other missing pieces\n",
        "#add data processing step to convert data e.g language to input tensor\n",
        "#add decoder\n",
        "\n",
        "#model taken from https://data-science-blog.com/blog/2021/04/07/multi-head-attention-mechanism/\n",
        "def create_padding_mask(seq):\n",
        "  seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
        "\n",
        "  # add extra dimensions to add the padding\n",
        "  # to the attention logits.\n",
        "  return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "def create_look_ahead_mask(size):\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "  return mask  # (seq_len, seq_len)\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead) \n",
        "  but it must be broadcastable for addition.\n",
        "\n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable \n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "  Returns:\n",
        "    output, attention_weights\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  # scale matmul_qk\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return output, attention_weights\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\n",
        "    Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "    \"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, v, k, q, mask):\n",
        "    print(\"Inside 'MultiHeadAttention' class...\")\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    print()\n",
        "    print(\"The shape of 'q' is \" + str(q.shape))\n",
        "    print(\"The shape of 'k' is \" + str(k.shape))\n",
        "    print(\"The shape of 'v' is \" + str(v.shape))\n",
        "    \n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "    \n",
        "    print()\n",
        "    print(\"After passing 'q', 'k', 'v' through densely connected layers....\")\n",
        "    print(\"The shape of 'q' is \" + str(q.shape))\n",
        "    print(\"The shape of 'k' is \" + str(k.shape))\n",
        "    print(\"The shape of 'v' is \" + str(v.shape))\n",
        "\n",
        " \n",
        "    q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "    \n",
        "    print()\n",
        "    print(\"After splitting the heads....\")\n",
        "    print(\"The shape of 'q' is \" + str(q.shape))\n",
        "    print(\"The shape of 'k' is \" + str(k.shape))\n",
        "    print(\"The shape of 'v' is \" + str(v.shape))\n",
        "\n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "\n",
        "    \n",
        "    print()\n",
        "    print(\"The shape of 'attention_weights' is \" + str(attention_weights.shape))\n",
        "\n",
        "\n",
        "    print(\"The shape of 'scaled_attention' is \" + str(scaled_attention.shape))\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "    \n",
        "    print()\n",
        "    print(\"After transposing....\")\n",
        "    print(\"The shape of 'scaled_attention' is \" + str(scaled_attention.shape))\n",
        "    concat_attention = tf.reshape(scaled_attention, \n",
        "                                  (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "    \n",
        "    print()\n",
        "    print(\"The shape of 'concat_attention' is \" + str(concat_attention.shape))\n",
        "    \n",
        "    output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "    print()\n",
        "    print(\"The shape of 'output' is \" + str(output.shape))\n",
        "\n",
        "    return output, attention_weights\n",
        "\n",
        "#calculated Attn = softmax((QK^T / sqrt(d_k))) V where Q, K, V are query, key, and value matrices, d_k is dimnesionality of a key, and K^T is key transpose matrix\n",
        "def scaled_dot_product_attention_mine(K: Tensor, Q: Tensor, V: Tensor): \n",
        "  V = tf.cast(V, tf.float32)\n",
        "  K = tf.cast(K, tf.float32)\n",
        "  Q = tf.cast(Q, tf.float32)\n",
        "  K_t = tf.transpose(K)\n",
        "  dot = matmul(Q, K_t)\n",
        "  d_k = K.shape[-1]\n",
        "  print(d_k)\n",
        "  print(dot)\n",
        "  dot = dot / d_k**(-0.5)\n",
        "  soft = softmax(dot)\n",
        "  return matmul(soft, V)\n",
        "\n",
        "\n",
        "#The feedforward sublayer combined with the residual connection\n",
        "#Input size is d_model, output size is d_model, hidden layer has dimensions d_hidden\n",
        "#ouput is calculated using:   ff(x) = ReLu(W1 * x + b_1) W_2 + b_2\n",
        "class FeedForward(tf.keras.layers.Layer): \n",
        "\n",
        "  def __init__(self, d_model, d_hidden) -> None:\n",
        "    super().__init__()\n",
        "\n",
        "    self.layer = tf.keras.Sequential(\n",
        "        [tf.keras.layers.Dense(d_hidden, activation='relu'), \n",
        "          tf.keras.layers.Dense(d_model)\n",
        "         ]\n",
        "    )\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "    self.Add = tf.keras.layers.Add()\n",
        "\n",
        "\n",
        "  def call(self, x): \n",
        "    x = self.Add([self.layer(x), x])\n",
        "    x = self.layer_norm(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "#The base class upon which multihead attention is built\n",
        "class attentionhead(tf.keras.layers.Layer): \n",
        "\n",
        "  def __init__(self, d_in: int, d_k: int, d_q: int): \n",
        "    super().__init__()\n",
        "    self.q = tf.keras.layers.Dense(d_q, input_shape = (d_in, ), activation=None)\n",
        "    self.k = tf.keras.layers.Dense(d_k, input_shape = (d_in, ), activation=None)\n",
        "    self.v = tf.keras.layers.Dense(d_k, input_shape = (d_in, ), activation=None)\n",
        "    \n",
        "\n",
        "  def call(self, key: Tensor, query: Tensor, value: Tensor):\n",
        "    return scaled_dot_product_attention_mine(self.k(key), self.q(query), self.v(value))\n",
        "     \n",
        "\n",
        "\n",
        "\n",
        "# each of the input-dimensional keys, values, and queries are projected to\n",
        "# key dimensions through a linear layer. Attention is performed on the transformed \n",
        "# tensors, and the transformed tensors are then projected again.  \n",
        "class multiheadattention(tf.keras.layers.Layer): \n",
        "\n",
        "  def __init__(self, num_heads: int, dim_in: int, dim_key: int, dim_query: int, mask: bool = False) -> None:\n",
        "    super().__init__()\n",
        "    #how to define a module list?\n",
        "    self.heads = [attentionhead(dim_in, dim_key, dim_query) for _ in range(num_heads)]\n",
        "    self.lin = tf.keras.layers.Dense(3 * dim_key, input_shape=(dim_in, ))\n",
        "#    self.lin = tf.keras.layers.Dense(num_heads * dim_key, input_shape=(dim_in, ))\n",
        "\n",
        "    #todo: figure out masking\n",
        "    if mask: \n",
        "      pass\n",
        "\n",
        "\n",
        "  def call(self, key: Tensor, query: Tensor, value: Tensor) -> Tensor:\n",
        "    return self.lin(\n",
        "        tf.concat([head(key, query, value) for head in self.heads], axis=-1, name=\"linear\")\n",
        "    )\n",
        "  \n",
        "#mha, add and norm -> dense layer, add and norm\n",
        "#dropout applied here since EncoderLayer combines all the sublayers\n",
        "class EncoderLayer(tf.keras.layers.Layer): \n",
        "  def __init__(self, d_model, num_heads, d_hidden, dropout=0.3) -> None:\n",
        "     super().__init__()\n",
        "     self.d_k = max(d_model // num_heads, 1)\n",
        "     self.mha = multiheadattention(num_heads, d_model, self.d_k, self.d_k, False)\n",
        "     self.ff = FeedForward(d_model, d_hidden)\n",
        "     self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "     self.Add = tf.keras.layers.Add()\n",
        "     self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "  def call(self, input): \n",
        "    #how to do batch normalization after mha?\n",
        "    K = tf.convert_to_tensor(input.numpy())\n",
        "    Q = tf.convert_to_tensor(input.numpy())\n",
        "    V = tf.convert_to_tensor(input.numpy())\n",
        "\n",
        "    x = self.mha(K, Q, V)\n",
        "    x = self.Add([x, input])\n",
        "    x = self.dropout(x)\n",
        "    x = self.layer_norm(x)\n",
        "    x = self.ff(x)\n",
        "    x = self.dropout(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "def split_heads(x, num_heads, batch_dimensions): \n",
        "  dim_model, dim_seq = x.shape\n",
        "\n",
        "  \n",
        "\n",
        "class DecoderLayer(tf.keras.layers.Layer) :\n",
        "  def __init__(self, d_model, num_heads, d_hidden, dropout=0.3): \n",
        "\n",
        "    self.d_k = tf.max(d_model // num_heads, 1)\n",
        "\n",
        "    self.masked_mha = multiheadattention(num_heads, d_model, self.d_k, self.d_k, mask=True)\n",
        "    self.mha = multiheadattention(num_heads, d_model, self.d_k, self.d_k, mask=True)\n",
        "    self.ff = FeedForward(d_model, d_hidden)\n",
        "    self.layer_norm = tf.keras.layers.LayerNormalization()\n",
        "    self.Add = tf.keras.layers.Add()\n",
        "    self.dropout = tf.keras.layers.Dropout(dropout)\n",
        "\n",
        "  \n",
        "  def call(self, input, cross_input):\n",
        "    K = tf.convert_to_tensor(input.numpy())\n",
        "    Q = tf.convert_to_tensor(cross_input.numpy())\n",
        "    V = tf.convert_to_tensor(cross_input.numpy())\n",
        "\n",
        "    #first sub layer with masking\n",
        "    x = self.masked_mha(K, Q, V)\n",
        "    x = self.Add([x, input])\n",
        "    x = self.dropout(x)\n",
        "    x = self.layer_norm(x)\n",
        "\n",
        "    #second without masking\n",
        "    x2 = tf.convert_to_tensor(x.numpy())\n",
        "    x = self.mha(K, K, K)\n",
        "    x = self.Add([x, x2])\n",
        "    x = self.dropout(x)\n",
        "    x = self.layer_norm(x)\n",
        "\n",
        "    #third sublayer\n",
        "    x = self.ff(x)\n",
        "    x = self.dropout(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#input is a tensor that is fed into the transformer and \n",
        "#positional encoding is added to it\n",
        "def create_positional_encoding(dim_model, dim_seq): \n",
        "  pos = tf.range(0, dim_model, 2, dtype=tf.float32).numpy()\n",
        "  pos = pos.reshape(1, -1, 1)\n",
        "  pos = tf.convert_to_tensor(pos)\n",
        "  dim = tf.range(0, dim_seq, dtype=tf.float32).numpy()\n",
        "  dim = dim.reshape(1, 1, -1)\n",
        "  dim = tf.convert_to_tensor(dim)\n",
        "  phase = pos / 1e4**(dim / dim_model)\n",
        "  return tf.where(dim%2 == 0, tf.sin(phase), tf.cos(phase))\n",
        "\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer): \n",
        "  def __init__(self, d_model, num_heads, d_hidden): \n",
        "    super().__init__()\n",
        "    self.l = [EncoderLayer(d_model, num_heads, d_hidden) for _ in range(6)]\n",
        "\n",
        "  def call(self, input: Tensor):\n",
        "    for lay in self.l: \n",
        "      input = lay(input)\n",
        "    return input\n",
        "\n",
        "\n",
        "class Decoder(tf.keras.layers.Layer): \n",
        "  def __init__(self, d_model, num_heads, d_hidden):\n",
        "    super().__init__()\n",
        "    self.l = [Decoder(self, d_model, num_heads, d_hidden) for _ in range(6)]\n",
        "\n",
        "    #do we need Input layer for the final linear layer?\n",
        "    self.linear = tf.keras.layers.Dense(d_model, activation='softmax')\n",
        "\n",
        "  \n",
        "  def call(self, input: Tensor): \n",
        "\n",
        "    x = input\n",
        "    for layer in self.l: \n",
        "      x = layer(x)\n",
        "    \n",
        "    return self.linear(x)\n",
        "\n",
        "    \n",
        "\n",
        "#print(input_tensor.shape)\n",
        "#print(rslt.shape)"
      ],
      "metadata": {
        "id": "NZ9oZbW5im5h",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "e6601fb4-7494-4ebb-ad42-7cf3fa7b3dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-1a6942f95002>\"\u001b[0;36m, line \u001b[0;32m318\u001b[0m\n\u001b[0;31m    #print(rslt.shape)\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d_model = 192\n",
        "num_heads = 3\n",
        "d_hidden = 1024\n",
        "enc = Encoder(d_model, num_heads, d_hidden)\n",
        "\n",
        "\n",
        "#create data\n",
        "input = tf.random.normal([192])\n",
        "input = tf.expand_dims(input, axis=0)\n",
        "print(input)\n",
        "print(enc(input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIJXfe1xaC6o",
        "outputId": "9245545e-bd78-4102-e460-bdb51e77c870"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[-0.32209584 -0.06927506 -0.57095814  0.7766935  -0.7868154  -0.39203525\n",
            "  -0.04428896 -0.60786307  0.05801601 -0.2546975  -0.92768675  0.72436327\n",
            "   1.1825615   1.2638073   1.2687995   0.9103692   1.298516    1.9871203\n",
            "  -0.09246769  1.6756735  -0.9789442   1.2061933  -0.652072    0.5392815\n",
            "   1.8566234  -0.41659877  1.5074799  -1.5703791  -0.23919009  0.53183734\n",
            "  -0.5769419  -0.7483095   0.73877513 -1.1111399  -0.14601016 -0.40166846\n",
            "  -0.55308616 -0.23467162  1.1752588   0.2001493   0.48362836 -0.6464562\n",
            "  -0.92099637  2.1429296   0.55001235  0.5439586   1.4686579   1.433936\n",
            "   1.2073331  -0.64747274  0.48129022  0.6143392  -1.5393376  -0.31541872\n",
            "  -0.6150933  -1.0597662  -0.13111484 -1.1910559   0.4392466   0.42834628\n",
            "   0.53299004 -0.3839335   2.7596025   1.15551     0.37441167 -0.20678559\n",
            "   0.6748126  -0.76579565 -0.1849377   2.021187   -1.0069524   1.6959141\n",
            "   1.3094616  -0.81022644 -0.8127374  -0.9872704  -0.9345691  -0.137123\n",
            "  -0.17527446  0.59964484  1.3765988  -0.11255731  0.7590117  -1.3655328\n",
            "   0.19977254  1.8589906   0.34119388 -1.3499875   0.08840901  0.11932544\n",
            "   1.0996103  -0.7663755   0.9899982   0.47683093  0.3469093  -0.6320553\n",
            "   1.0459585  -0.3214882  -0.59488624 -0.28561124 -0.70354927  0.23316568\n",
            "   1.4610199   0.6463376  -0.96713114 -1.7617569   1.3889704   1.2424117\n",
            "  -0.02402372 -0.44781688  1.0688353  -0.54830736 -1.8943708  -0.44423378\n",
            "   1.3626511  -0.6909717   0.35313323 -0.79082066  0.6627702  -1.1540353\n",
            "   1.0040553  -0.01194391 -1.4553083   0.9125585   0.20977406  1.0972015\n",
            "  -0.7662617  -0.7587637   0.606284   -0.0622881  -0.04933337  1.1493484\n",
            "   0.42474738  0.6951406   0.9301501  -1.8476187   0.18580164 -0.45702368\n",
            "   0.59848493  0.50903434 -1.4154544   0.96713406 -0.4321491  -0.194127\n",
            "  -0.21038029 -1.0145593   0.76683307  1.7715935  -0.9319997  -1.8410422\n",
            "  -1.0339315  -0.29790854 -0.97625893 -0.77469933  0.6847983   0.00687234\n",
            "   0.4858014  -0.43007907 -0.04967319 -1.8051534   2.5866795  -0.84846354\n",
            "   0.08603811 -0.1877974   1.4814359   1.169163   -1.8942494  -1.6744308\n",
            "  -0.45481932 -0.21640275 -0.7185269  -0.20187686 -0.57583016 -0.5035517\n",
            "  -0.14742345 -0.6161646   1.7235869  -0.02475898  0.70443976  1.0879215\n",
            "  -0.58096665 -0.18786143 -0.3718451  -2.3265991   0.96881855  1.0824082\n",
            "  -0.8854458   0.4409729  -1.2036728   1.577479    0.02800804  0.73680604]], shape=(1, 192), dtype=float32)\n",
            "64\n",
            "tf.Tensor([[9.436506]], shape=(1, 1), dtype=float32)\n",
            "64\n",
            "tf.Tensor([[17.884136]], shape=(1, 1), dtype=float32)\n",
            "64\n",
            "tf.Tensor([[-8.460153]], shape=(1, 1), dtype=float32)\n",
            "64\n",
            "tf.Tensor([[0.6193707]], shape=(1, 1), dtype=float32)\n",
            "64\n",
            "tf.Tensor([[4.4259005]], shape=(1, 1), dtype=float32)\n",
            "64\n",
            "tf.Tensor([[19.730251]], shape=(1, 1), dtype=float32)\n",
            "64\n",
            "tf.Tensor([[9.251575]], shape=(1, 1), dtype=float32)\n",
            "64\n",
            "tf.Tensor([[2.7995834]], shape=(1, 1), dtype=float32)\n",
            "64\n",
            "tf.Tensor([[1.9943893]], shape=(1, 1), dtype=float32)\n",
            "64\n",
            "tf.Tensor([[14.890036]], shape=(1, 1), dtype=float32)\n",
            "64\n",
            "tf.Tensor([[-20.437506]], shape=(1, 1), dtype=float32)\n",
            "64\n",
            "tf.Tensor([[-4.550729]], shape=(1, 1), dtype=float32)\n",
            "64\n",
            "tf.Tensor([[-11.210273]], shape=(1, 1), dtype=float32)\n",
            "64\n",
            "tf.Tensor([[15.936221]], shape=(1, 1), dtype=float32)\n",
            "64\n",
            "tf.Tensor([[-9.154795]], shape=(1, 1), dtype=float32)\n",
            "64\n",
            "tf.Tensor([[-11.180836]], shape=(1, 1), dtype=float32)\n",
            "64\n",
            "tf.Tensor([[0.53582597]], shape=(1, 1), dtype=float32)\n",
            "64\n",
            "tf.Tensor([[-6.5293264]], shape=(1, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-6.05439723e-01 -1.35703218e+00 -3.01445395e-01 -1.96690723e-01\n",
            "   1.81708187e-02 -7.01963842e-01 -2.38121882e-01  1.57911670e+00\n",
            "  -1.29315650e+00  8.07836592e-01  3.01937670e-01 -2.07515454e+00\n",
            "  -9.87449110e-01  8.15072060e-01  1.39804661e+00 -1.05547297e+00\n",
            "  -8.21059525e-01  4.11164314e-01 -1.56209278e+00  3.74673635e-01\n",
            "   3.59713525e-01  7.60176301e-01  1.71698555e-01 -5.62759519e-01\n",
            "  -4.29680407e-01  1.23277426e+00 -4.19596918e-02 -1.86208799e-01\n",
            "   1.87793389e-01  5.65326869e-01  1.35078263e+00  2.76247621e-01\n",
            "  -1.92350197e+00  2.37778351e-01  1.75140396e-01  7.95464590e-02\n",
            "  -1.15650654e+00 -1.82803237e+00 -1.09904742e+00  2.95243829e-01\n",
            "   8.64591897e-01 -9.94846225e-01 -1.10634007e-01  1.48104737e-03\n",
            "  -2.59692937e-01 -2.26025391e+00  4.57753837e-01  1.29242873e+00\n",
            "   7.69870281e-01 -8.99028122e-01 -4.21423942e-01 -6.90942466e-01\n",
            "   4.28884476e-01 -9.71653223e-01  2.39914924e-01 -2.70116329e-01\n",
            "  -1.73615605e-01  1.13333690e+00 -9.07956719e-01  1.97114205e+00\n",
            "  -8.80316317e-01 -1.78671435e-01  2.91938901e-01  3.09011173e+00\n",
            "  -2.56250650e-01 -1.78936999e-02  1.14118254e+00 -6.29380643e-01\n",
            "  -9.18739676e-01 -1.23167269e-01  2.10278392e+00 -7.72965133e-01\n",
            "  -3.04862589e-01 -4.75316644e-01  3.20870310e-01 -1.62972081e+00\n",
            "   5.91660321e-01 -1.02788770e+00 -1.10337746e+00  2.58922517e-01\n",
            "  -5.85954040e-02 -7.96952605e-01  1.70629978e-01  7.45117426e-01\n",
            "   1.42242813e+00  1.11472738e+00 -2.52235949e-01  6.26006603e-01\n",
            "  -7.84860253e-01  1.79082322e+00 -5.16747296e-01  8.08868825e-01\n",
            "  -1.60118771e+00  1.00944293e+00  1.10066688e+00 -4.09912258e-01\n",
            "   1.48068309e+00 -5.96341848e-01  7.55223095e-01 -4.57470328e-01\n",
            "  -2.81624794e-01  1.74997294e+00 -8.01302910e-01 -6.65850878e-01\n",
            "   1.08360600e+00 -1.15357077e+00 -9.92478728e-01 -2.74862528e-01\n",
            "   1.68230578e-01 -1.08775687e+00  3.49216402e-01  2.65791923e-01\n",
            "  -1.68407643e+00 -1.14817925e-01 -2.04610872e+00  1.24533892e+00\n",
            "   6.06082261e-01 -1.08593255e-01 -3.09732735e-01  1.42568183e+00\n",
            "   1.59789550e+00 -1.81744409e+00  2.80777246e-01 -6.13613844e-01\n",
            "  -9.33725476e-01  1.51331758e+00 -6.56120360e-01 -3.74793649e-01\n",
            "   1.61351264e+00  4.40184437e-02  3.78781170e-01  5.92916131e-01\n",
            "  -3.06050241e-01  1.67021179e+00 -8.03395748e-01  5.24701357e-01\n",
            "   2.58880877e+00 -1.52182651e+00  6.95276320e-01  3.36208969e-01\n",
            "   1.23507309e+00  4.22693253e-01  4.80600536e-01  9.80699480e-01\n",
            "   1.01173431e-01 -1.50837266e+00 -1.33461785e+00 -2.17891335e-01\n",
            "  -8.84837508e-01  1.23027694e+00  1.34712195e+00 -1.05574325e-01\n",
            "  -2.24240795e-01 -8.16254094e-02  4.72856104e-01  9.88123238e-01\n",
            "  -5.97617149e-01  3.63581665e-02 -4.58378941e-01  5.73345125e-01\n",
            "  -5.14120096e-04 -5.57894051e-01 -4.91956562e-01  6.75503016e-01\n",
            "   4.53513682e-01 -1.32565156e-01 -1.04072876e-01  1.44523740e+00\n",
            "  -7.51988411e-01  3.92990917e-01 -9.02285457e-01  1.12962611e-01\n",
            "  -1.10566117e-01  9.69546497e-01  3.46825831e-02  2.24816948e-01\n",
            "  -1.04606414e+00 -2.00536275e+00  8.10449481e-01  1.51898766e+00\n",
            "   2.82013464e+00 -1.40622957e-02 -7.18026236e-02  1.73523986e+00\n",
            "  -1.79845047e+00 -1.20163333e+00  6.54544652e-01 -2.11814821e-01\n",
            "  -5.46049595e-01 -4.23731685e-01 -2.21248841e+00 -1.10701442e+00]], shape=(1, 192), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len, dim_model = 100, 200\n",
        "pos = torch.arange(seq_len, dtype=torch.float).reshape(1, -1, 1)\n",
        "dim = torch.arange(dim_model, dtype=torch.float).reshape(1, 1, -1)\n",
        "phase = pos / (1e4 ** (dim / dim_model))\n",
        "print(torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase)))\n",
        "\n",
        "print(create_positional_encoding(dim_model, seq_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8T4uUCMaH-o",
        "outputId": "8bd66c90-8618-49aa-ee78-2dbb863587fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
            "           0.0000e+00,  1.0000e+00],\n",
            "         [ 8.4147e-01,  5.7761e-01,  7.9074e-01,  ...,  1.0000e+00,\n",
            "           1.0965e-04,  1.0000e+00],\n",
            "         [ 9.0930e-01, -3.3272e-01,  9.6811e-01,  ...,  1.0000e+00,\n",
            "           2.1930e-04,  1.0000e+00],\n",
            "         ...,\n",
            "         [ 3.7961e-01, -4.2693e-02,  4.7983e-01,  ...,  9.9994e-01,\n",
            "           1.0636e-02,  9.9995e-01],\n",
            "         [-5.7338e-01,  7.9091e-01,  9.8749e-01,  ...,  9.9994e-01,\n",
            "           1.0745e-02,  9.9995e-01],\n",
            "         [-9.9921e-01,  9.5637e-01,  7.2918e-01,  ...,  9.9994e-01,\n",
            "           1.0855e-02,  9.9995e-01]]])\n",
            "tf.Tensor(\n",
            "[[[ 0.          1.          0.         ...  1.          0.\n",
            "    1.        ]\n",
            "  [ 0.9092974  -0.33272225  0.9681094  ...  0.99973637  0.0219278\n",
            "    0.9997807 ]\n",
            "  [-0.7568025  -0.7785918  -0.48507634 ...  0.9989456   0.04384506\n",
            "    0.9991229 ]\n",
            "  ...\n",
            "  [-0.7023863  -0.99635464  0.8419628  ... -0.6104443   0.8491771\n",
            "   -0.44451517]\n",
            "  [ 0.9395301   0.25106487  0.31139567 ... -0.62847     0.8373927\n",
            "   -0.4631762 ]\n",
            "  [-0.07957859  0.82928336 -0.9979885  ... -0.6461642   0.8252056\n",
            "   -0.48163387]]], shape=(1, 100, 100), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#TESTING CELL\n",
        "def check_attention_head():  \n",
        "  K = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
        "  V = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
        "  Q = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
        "  \n",
        "  print(\"Running  attention function...\")\n",
        "  head = attentionhead(3, K.shape[-1], Q.shape[-1])\n",
        "  print(head(K, Q, V))\n",
        "\n",
        "\n",
        "def self_attention_check(): \n",
        "  K = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
        "  V = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
        "  Q = tf.constant([[1, 2, 3], [4, 5, 6]])\n",
        "  \n",
        "  print(\"Running multi headattention function...\")\n",
        "  head = multiheadattention(3, 3, K.shape[-1], Q.shape[-1], False)\n",
        "  print(head(K, Q, V))\n",
        "  K = tf.cast(K, tf.int32)\n",
        "  Q = tf.cast(Q, tf.int32)\n",
        "  V = tf.cast(V, tf.int32)\n",
        "\n",
        "  #testing\n",
        "  temp_mha = MultiHeadAttention(d_model=K.shape[-1], num_heads=3)\n",
        "  out, attn = temp_mha(v=V, k=K, q=Q, mask=None)\n",
        "  print(out, attn)\n",
        "\n",
        "\n",
        "#todo: how to implement MultiHeadAttention from tensorflow??????\n",
        "#  layer = tf.keras.layers.MultiHeadAttention(num_heads=3, key_dim=6, use_bias=False)\n",
        "#  print(layer(Q, V, K))\n",
        "\n",
        "self_attention_check()"
      ],
      "metadata": {
        "id": "0ugosdPdnihn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24de014d-3f99-4c15-f511-73d5eee0e2f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running multi headattention function...\n",
            "3\n",
            "tf.Tensor(\n",
            "[[-0.4375267 -4.169918 ]\n",
            " [ 0.5893545 -4.651104 ]], shape=(2, 2), dtype=float32)\n",
            "3\n",
            "tf.Tensor(\n",
            "[[-1.343729  -3.0636737]\n",
            " [-1.9823676 -5.6841383]], shape=(2, 2), dtype=float32)\n",
            "3\n",
            "tf.Tensor(\n",
            "[[ 7.5099554 11.30494  ]\n",
            " [22.314337  36.26994  ]], shape=(2, 2), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[-3.583356    2.7362645   3.0014017  -6.147074   -3.6801023  -4.738505\n",
            "  -4.1570363  -4.6377773   0.02600089]\n",
            " [-3.6483214   2.5922365   3.0082812  -6.184674   -3.5438685  -4.6698055\n",
            "  -4.180213   -4.4322987   0.08446279]], shape=(2, 9), dtype=float32)\n",
            "Inside 'MultiHeadAttention' class...\n",
            "\n",
            "The shape of 'q' is (2, 3)\n",
            "The shape of 'k' is (2, 3)\n",
            "The shape of 'v' is (2, 3)\n",
            "\n",
            "After passing 'q', 'k', 'v' through densely connected layers....\n",
            "The shape of 'q' is (2, 3)\n",
            "The shape of 'k' is (2, 3)\n",
            "The shape of 'v' is (2, 3)\n",
            "\n",
            "After splitting the heads....\n",
            "The shape of 'q' is (2, 3, 1, 1)\n",
            "The shape of 'k' is (2, 3, 1, 1)\n",
            "The shape of 'v' is (2, 3, 1, 1)\n",
            "\n",
            "The shape of 'attention_weights' is (2, 3, 1, 1)\n",
            "The shape of 'scaled_attention' is (2, 3, 1, 1)\n",
            "\n",
            "After transposing....\n",
            "The shape of 'scaled_attention' is (2, 1, 3, 1)\n",
            "\n",
            "The shape of 'concat_attention' is (2, 1, 3)\n",
            "\n",
            "The shape of 'output' is (2, 1, 3)\n",
            "tf.Tensor(\n",
            "[[[ 1.4624565 -1.3238031  1.4282923]]\n",
            "\n",
            " [[ 5.46726   -3.1998086  4.534686 ]]], shape=(2, 1, 3), dtype=float32) tf.Tensor(\n",
            "[[[[1.]]\n",
            "\n",
            "  [[1.]]\n",
            "\n",
            "  [[1.]]]\n",
            "\n",
            "\n",
            " [[[1.]]\n",
            "\n",
            "  [[1.]]\n",
            "\n",
            "  [[1.]]]], shape=(2, 3, 1, 1), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Greg's model\n",
        "\n",
        "# Define core layers\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers, initializers, activations\n",
        "import numpy as np\n",
        "\n",
        "class FeedForwardNetwork(layers.Layer):\n",
        "    def __init__(self, inner_dim=2048, outer_dim=512):\n",
        "        super().__init__()\n",
        "        self.inner_dim = inner_dim\n",
        "        self.outer_dim = outer_dim\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        self.batch_dim = input_shape[0]\n",
        "        self.input_dim = input_shape[1:]\n",
        "        \n",
        "        self.d1 = layers.Dense(self.inner_dim, activation=\"relu\")\n",
        "        self.d2 = layers.Dense(self.outer_dim, activation=None)\n",
        "    \n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        x = inputs\n",
        "        x = self.d1(x)\n",
        "        x = self.d2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Modified from www.tensorflow.org/text/tutorials/transformer\n",
        "def positional_encoding(length, depth):\n",
        "    depth = depth // 2\n",
        "    \n",
        "    positions = np.arange(length)[:, np.newaxis]\n",
        "    depths = np.arange(depth)[np.newaxis, :] / depth\n",
        "    \n",
        "    angle_rates = 1 / (10000 ** depths)\n",
        "    angle_rads = positions * angle_rates\n",
        "    \n",
        "    # Is this right?\n",
        "    #pos_encoding = np.concatenate(\n",
        "    #    [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "    #    axis=-1,\n",
        "    #)\n",
        "    pos_encoding = np.stack(\n",
        "        [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "        axis=-1,\n",
        "    )\n",
        "    pos_encoding = np.reshape(pos_encoding, (length, depth * 2))\n",
        "    pos_encoding = tf.constant(pos_encoding, dtype=tf.float32)\n",
        "    return pos_encoding\n",
        "\n",
        "\n",
        "class PositionalEncoding(layers.Layer):\n",
        "    def __init__(self, seq_size, d_model):\n",
        "        super().__init__()\n",
        "        self.seq_dim = seq_size\n",
        "        self.d_model = d_model\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        pass\n",
        "    \n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        seq_dim, batch_dim, model_dim = inputs.shape\n",
        "        \n",
        "        #pos = tf.expand_dims(tf.ones(0, seq_dim), axis=-1)\n",
        "        # (seq_dim, 1)\n",
        "        #pos = tf.one_hot(tf.range(0, seq_dim), seq_dim)\n",
        "        # TODO: check this\n",
        "        pos = tf.ones(shape=(self.seq_dim, 1))\n",
        "        pe = positional_encoding(self.seq_dim, model_dim)\n",
        "        pos = pos * positional_encoding(self.seq_dim, model_dim)\n",
        "        # (seq_dim, model_dim)\n",
        "        \n",
        "        pos = tf.expand_dims(pos, axis=-2)\n",
        "        # (seq_dim, 1, model_dim)\n",
        "        \n",
        "        # pos broadcasts to (seq_dim, batch_dim, model_dim)\n",
        "        inputs += pos\n",
        "        \n",
        "        return inputs\n",
        "\n",
        "\n",
        "class MultiHeadAttention(layers.Layer):\n",
        "    def __init__(self, d_key=64, d_value=64, d_model=512, mask=False, num_heads=8, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.d_key = d_key\n",
        "        self.d_value = d_value\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.mask = mask\n",
        "        \n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.WQ = self.add_weight(\n",
        "            name=\"WQ\",\n",
        "            shape=(self.num_heads, self.d_model, self.d_key),\n",
        "            initializer=initializers.GlorotNormal(),\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.WK = self.add_weight(\n",
        "            name=\"WK\",\n",
        "            shape=(self.num_heads, self.d_model, self.d_key),\n",
        "            initializer=initializers.GlorotNormal(),\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.WV = self.add_weight(\n",
        "            name=\"WV\",\n",
        "            shape=(self.num_heads, self.d_model, self.d_value),\n",
        "            initializer=initializers.GlorotNormal(),\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.WO = self.add_weight(\n",
        "            name=\"WO\",\n",
        "            shape=(self.num_heads * self.d_value, self.d_model),\n",
        "            initializer=initializers.GlorotNormal(),\n",
        "            trainable=True,\n",
        "        )\n",
        "        self.dropout_layer = layers.Dropout(self.dropout_rate)\n",
        "\n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        dropout_rate = 0.1\n",
        "        \n",
        "        # Extract inputs, and reshape for broadcasting to multiple heads\n",
        "        if len(inputs) == 3:\n",
        "            Q, K, V = inputs\n",
        "            has_mask = False\n",
        "        elif len(inputs) == 4:\n",
        "            Q, K, V, mask = inputs\n",
        "            has_mask = True\n",
        "\n",
        "        # expand for heads, and into row vector\n",
        "        Q = tf.expand_dims(Q, axis=-2)\n",
        "        K = tf.expand_dims(K, axis=-2)\n",
        "        V = tf.expand_dims(V, axis=-2)\n",
        "        Q = tf.expand_dims(Q, axis=-2)\n",
        "        K = tf.expand_dims(K, axis=-2)\n",
        "        V = tf.expand_dims(V, axis=-2)\n",
        "        # (seq, batch, 1, 1, d_model)\n",
        "\n",
        "        # Project inputs\n",
        "        Q = tf.matmul(Q, self.WQ)\n",
        "        K = tf.matmul(K, self.WK)\n",
        "        V = tf.matmul(V, self.WV)\n",
        "        \n",
        "        Q = tf.squeeze(Q, axis=-2)\n",
        "        K = tf.squeeze(K, axis=-2)\n",
        "        V = tf.squeeze(V, axis=-2)\n",
        "        # (seq, batch, heads, d_key / d_value)\n",
        "        \n",
        "        # Calculate attention\n",
        "        attention_logits = (Q * K) / tf.math.sqrt(tf.cast(K.shape[-1], tf.float32))\n",
        "        attention_logits = tf.math.reduce_sum(attention_logits, axis=-1, keepdims=True)\n",
        "        # (seq, batch, heads, 1)\n",
        "        \n",
        "        if has_mask:\n",
        "            # (seq,)\n",
        "            mask = tf.expand_dims(mask, axis=-1)\n",
        "            mask = tf.expand_dims(mask, axis=-1)\n",
        "            mask = tf.expand_dims(mask, axis=-1)\n",
        "            # (seq, 1, 1, 1)\n",
        "            # mask should be non-zero to retain, zero to mask\n",
        "            # set non-zero to zero, zero to -inf, then add\n",
        "            inf_mask = tf.where(tf.cast(mask, tf.bool), tf.zeros(shape=(1,)), tf.constant(float(\"-inf\"), shape=(1,)))\n",
        "            mask_attention_logits = tf.add(attention_logits, inf_mask)\n",
        "        else:\n",
        "            mask_attention_logits = attention_logits\n",
        "        \n",
        "        # (seq, batch, heads, 1)\n",
        "        attention_scores = activations.softmax(mask_attention_logits, axis=0)\n",
        "        # (seq, batch, heads, 1)\n",
        "\n",
        "        attention_values = self.dropout_layer(attention_scores) * V\n",
        "        # (seq, batch, heads, d_value)\n",
        "        \n",
        "        # Place heads before values, reshape to concatenate\n",
        "        seq_dim, batch_dim, head_dim, value_dim = attention_values.shape.as_list()\n",
        "        reshaped_attention_values = tf.reshape(\n",
        "            attention_values,\n",
        "            shape=(seq_dim, batch_dim, head_dim * value_dim)\n",
        "        )\n",
        "        # (seq, batch, heads * d_value)\n",
        "        multihead_attention = tf.matmul(reshaped_attention_values, self.WO)\n",
        "        # (seq, batch, d_model)\n",
        "        \n",
        "        return multihead_attention\n",
        "\n",
        "class EncoderLayer(layers.Layer):\n",
        "    def __init__(self, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        seq_dim, batch_dim, model_dim = input_shape\n",
        "        self.MA = MultiHeadAttention(d_model=model_dim)\n",
        "\n",
        "        self.LN1 = layers.LayerNormalization(axis=-1)\n",
        "        self.LN2 = layers.LayerNormalization(axis=-1)\n",
        "        self.FFN = FeedForwardNetwork(outer_dim=model_dim)\n",
        "        self.dropout_layer1 = layers.Dropout(self.dropout_rate)\n",
        "        self.dropout_layer2 = layers.Dropout(self.dropout_rate)\n",
        "    \n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        # MA Attention, dropout, add & norm,\n",
        "        # FFN, dropout, add & norm\n",
        "        x0 = inputs\n",
        "        x = x0\n",
        "        x = self.MA((x, x, x))\n",
        "        x = self.dropout_layer1(x)\n",
        "        x = self.LN1(x + x0)\n",
        "        x1 = x\n",
        "        x = self.FFN(x)\n",
        "        x = self.dropout_layer2(x)\n",
        "        x = self.LN2(x + x1)\n",
        "        return x\n",
        "\n",
        "class Encoder(layers.Layer):\n",
        "    def __init__(self, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        seq_dim, batch_dim, model_dim = input_shape\n",
        "        self.lay = []\n",
        "        for i in range(6):\n",
        "            self.lay.append(EncoderLayer(self.dropout_rate))\n",
        "    \n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        x = inputs\n",
        "        for i in range(6):\n",
        "            x = self.lay[i](x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class DecoderLayer(layers.Layer):\n",
        "    def __init__(self, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        (\n",
        "            (seq_dim, batch_dim, model_dim),\n",
        "            (input_seq_dim, input_batch_dim, input_model_dim),\n",
        "            mask_shape\n",
        "        ) = input_shape\n",
        "        self.MA = MultiHeadAttention(model_dim)\n",
        "        self.CrossAttention = MultiHeadAttention(model_dim)\n",
        "        self.FFN = FeedForwardNetwork(outer_dim=model_dim)\n",
        "        self.LN1 = layers.LayerNormalization(axis=-1)\n",
        "        self.LN2 = layers.LayerNormalization(axis=-1)\n",
        "        self.LN3 = layers.LayerNormalization(axis=-1)\n",
        "        self.dropout_layer1 = layers.Dropout(self.dropout_rate)\n",
        "        self.dropout_layer2 = layers.Dropout(self.dropout_rate)\n",
        "        self.dropout_layer3 = layers.Dropout(self.dropout_rate)\n",
        "        \n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        # MA Masked Self-Attention, dropout, add & norm,\n",
        "        # MA Cross Attention, dropout, add & norm,\n",
        "        # FFN, dropout, add & norm\n",
        "        x0, y, mask = inputs\n",
        "        x = x0\n",
        "        x = self.MA((x, x, x, mask))\n",
        "        x = self.dropout_layer1(x)\n",
        "        x = self.LN1(x + x0)\n",
        "        x1 = x\n",
        "        x = self.MA((x, y, y))\n",
        "        x = self.dropout_layer2(x)\n",
        "        x = self.LN2(x + x1)\n",
        "        x2 = x\n",
        "        x = self.FFN(x)\n",
        "        x = self.dropout_layer3(x)\n",
        "        x = self.LN3(x + x2)\n",
        "        return x\n",
        "\n",
        "class Decoder(layers.Layer):\n",
        "    def __init__(self, d_vocab, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.d_vocab = d_vocab\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        (seq_dim, batch_dim, model_dim), enc_shape, mask_shape = input_shape\n",
        "        self.lay = []\n",
        "        for i in range(6):\n",
        "            self.lay.append(DecoderLayer(self.dropout_rate))\n",
        "        self.linear = layers.Dense(self.d_vocab, activation=\"softmax\")\n",
        "        \n",
        "    \n",
        "    def call(self, inputs, *args, **kwargs):\n",
        "        x, y, mask = inputs\n",
        "        for i in range(6):\n",
        "            x = self.lay[i]((x, y, mask))\n",
        "        x = activations.softmax(self.linear(x), axis=-1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "yMrCURIsGYn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data and set constants\n",
        "\n",
        "import tensorflow.keras as keras\n",
        "import pathlib\n",
        "import random\n",
        "\n",
        "# Download dataset\n",
        "text_file = keras.utils.get_file(\n",
        "    fname=\"spa-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\"\n",
        "\n",
        "# Extract eng, spa, text_pairs\n",
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, spa = line.split(\"\\t\")\n",
        "    eng = eng.lower()\n",
        "    spa = spa.lower()\n",
        "    text_pairs.append((eng, spa))\n",
        "\n",
        "# Shuffle, train, val, test split\n",
        "random.shuffle(text_pairs)\n",
        "num_val_samples = int(0.15 * len(text_pairs))\n",
        "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
        "train_pairs = text_pairs[:num_train_samples]\n",
        "val_pairs = text_pairs[num_train_samples : num_train_samples + num_val_samples]\n",
        "test_pairs = text_pairs[num_train_samples + num_val_samples :]\n",
        "\n",
        "print(f\"{len(text_pairs)} total pairs\")\n",
        "print(f\"{len(train_pairs)} training pairs\")\n",
        "print(f\"{len(val_pairs)} validation pairs\")\n",
        "print(f\"{len(test_pairs)} test pairs\")\n",
        "\n",
        "# Set eng, spa train samples\n",
        "eng_samples = [text_pair[0] for text_pair in train_pairs]\n",
        "spa_samples = [text_pair[1] for text_pair in train_pairs]\n",
        "\n",
        "# Set model constants\n",
        "BATCH_SIZE = 1\n",
        "EPOCHS = 10  # This should be at least 10 for convergence\n",
        "STEPS_PER_EPOCH = 1\n",
        "MAX_SEQUENCE_LENGTH = 40\n",
        "ENG_VOCAB_SIZE = 10000\n",
        "SPA_VOCAB_SIZE = 10000\n",
        "\n",
        "KEY_DIM = 64\n",
        "EMBED_DIM = 512\n",
        "INTERMEDIATE_DIM = 2048\n",
        "NUM_HEADS = 8\n",
        "\n",
        "input_dim = ENG_VOCAB_SIZE\n",
        "output_dim = SPA_VOCAB_SIZE\n",
        "model_dim = 512\n",
        "SHARED_SEQUENCE_SIZE = 8\n",
        "input_seq_size = SHARED_SEQUENCE_SIZE\n",
        "output_seq_size = SHARED_SEQUENCE_SIZE\n",
        "# output_seq_size = 40\n",
        "# seq_size = output_seq_size\n"
      ],
      "metadata": {
        "id": "J3duqmDdETGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean and tokenize data\n",
        "\n",
        "import string\n",
        "from itertools import chain, repeat\n",
        "import numpy as np\n",
        "\n",
        "RESERVED_TOKENS = [\"<UNK> <START> <END> <PAD>\"]\n",
        "class StringLookup():\n",
        "    def __init__(self, vocab_size=10000):\n",
        "        self.lookup = {}\n",
        "        self.rlookup = {}\n",
        "        self.vocab = []\n",
        "        self.vocab_size = vocab_size\n",
        "        self.adapt(RESERVED_TOKENS)\n",
        "    \n",
        "    def adapt(self, texts):\n",
        "        for text in texts:\n",
        "            tokens = self.tokenize(text)\n",
        "            for token in tokens:\n",
        "                if self.lookup.get(token, None) is None:\n",
        "                    length = len(self.vocab)\n",
        "                    if length < self.vocab_size:\n",
        "                        self.vocab.append(token)\n",
        "                        self.lookup[token] = length\n",
        "                        self.rlookup[length] = token\n",
        "                    else:\n",
        "                        return\n",
        "                    \n",
        "    def convert(self, texts, seq_size=100):\n",
        "        if not isinstance(texts, list):\n",
        "            texts = [texts]\n",
        "        return [\n",
        "            [\n",
        "                self.lookup.get(token, 0)\n",
        "                for token in list(\n",
        "                    chain([\"<START>\"], self.tokenize(text)[:seq_size - 2], [\"<END>\"], repeat(\"<PAD>\", seq_size))\n",
        "                )[:seq_size]\n",
        "            ] for text in texts\n",
        "        ]\n",
        "    \n",
        "    def convert_one(self, text, seq_size=100):\n",
        "        return [\n",
        "            self.lookup.get(token, 0)\n",
        "                for token in list(\n",
        "                    chain([\"<START>\"], self.tokenize(text)[:seq_size - 2], [\"<END>\"], repeat(\"<PAD>\", seq_size))\n",
        "                )[:seq_size]\n",
        "        ]\n",
        "\n",
        "\n",
        "    def rconvert(self, vs):\n",
        "        if not isinstance(vs, list):\n",
        "            vs = [vs]\n",
        "        return [\" \".join([self.rlookup.get(i, \"<UNK>\") for i in v]) for v in vs]\n",
        "    \n",
        "    def tokenize(self, text):\n",
        "        return text.split()\n",
        "    \n",
        "string.punctuation\n",
        "spanish_punctuation = string.punctuation + \"\"\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    punctuation_free = \"\".join([i for i in text if i not in spanish_punctuation])\n",
        "    return punctuation_free\n",
        "\n",
        "eng_samples_no_punc = [remove_punctuation(eng_sample) for eng_sample in eng_samples]\n",
        "spa_samples_no_punc = [remove_punctuation(spa_sample) for spa_sample in spa_samples]\n",
        "train_pairs_no_punc = [(remove_punctuation(e), remove_punctuation(s)) for e, s in train_pairs]\n",
        "val_pairs_no_punc = [(remove_punctuation(e), remove_punctuation(s)) for e, s in val_pairs]\n",
        "test_pairs_no_punc = [(remove_punctuation(e), remove_punctuation(s)) for e, s in test_pairs]\n",
        "\n",
        "en_lookup = StringLookup(vocab_size=ENG_VOCAB_SIZE)\n",
        "en_lookup.adapt(eng_samples_no_punc)\n",
        "\n",
        "es_lookup = StringLookup(vocab_size=SPA_VOCAB_SIZE)\n",
        "es_lookup.adapt(spa_samples_no_punc)\n",
        "\n",
        "train_pairs_int = [(en_lookup.convert(en, seq_size=input_seq_size)[0], es_lookup.convert(es, seq_size=output_seq_size)[0]) for en, es in train_pairs_no_punc]\n",
        "val_pairs_int = [(en_lookup.convert(en, seq_size=input_seq_size)[0], es_lookup.convert(es, seq_size=output_seq_size)[0]) for en, es in val_pairs_no_punc]\n",
        "test_pairs_int = [(en_lookup.convert(en, seq_size=input_seq_size)[0], es_lookup.convert(es, seq_size=output_seq_size)[0]) for en, es in test_pairs_no_punc]\n",
        "train_pairs_np = np.array(train_pairs_int)\n",
        "val_pairs_np = np.array(val_pairs_int)\n",
        "test_pairs_np = np.array(test_pairs_int)\n",
        "\n",
        "train_pairs_back = [(en_lookup.rconvert([en])[0], es_lookup.rconvert([es])[0]) for en, es in train_pairs_int]\n",
        "train_pairs_back"
      ],
      "metadata": {
        "id": "XJijH7M9EXp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RHXfZi0yEC_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0FNp2GiWEC8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pus-waNFEC4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k8j1mNTKECu3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}